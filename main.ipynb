{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from modules import Encoder, Decoder\n",
    "from loss import LPIPSWithDiscriminator\n",
    "from distribution import DiagonalGaussianDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAutoencoderKL(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 latent_dim = None, \n",
    "                 *, \n",
    "                 image_size=32, \n",
    "                 patch_size=4,\n",
    "                 in_channels=3, \n",
    "                 hidden_size=1152, \n",
    "                 depth=12, \n",
    "                 num_heads=6, \n",
    "                 mlp_ratio=4.0, \n",
    "                 num_classes=10, \n",
    "                 dropout_prob=0.1,\n",
    "                 z_channels=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if latent_dim is None:\n",
    "            self.latent_dim = (patch_size, patch_size, in_channels)\n",
    "        else:\n",
    "            self.latent_dim = latent_dim\n",
    "        \n",
    "        if z_channels is None:\n",
    "            self.z_channels = self.latent_dim[2]\n",
    "        else:\n",
    "            self.z_channels = z_channels\n",
    "        self.image_size = image_size\n",
    "        assert isinstance(latent_dim, tuple) and len(latent_dim) == 3, 'Latent_dim must be a tuple of length 3 in the form (H, W, C)'\n",
    "        #self.image_key = image_key\n",
    "        self.encoder = Encoder(self.latent_dim, image_size=image_size, patch_size=patch_size, in_channels=in_channels, hidden_size=hidden_size, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=num_classes, dropout_prob=dropout_prob)\n",
    "        self.decoder = Decoder(self.latent_dim, image_size=image_size, patch_size=patch_size, in_channels=in_channels, hidden_size=hidden_size, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=num_classes, dropout_prob=dropout_prob)\n",
    "        self.loss = LPIPSWithDiscriminator()\n",
    "\n",
    "        self.embed_dim = self.latent_dim[2]\n",
    "        self.quant_conv = torch.nn.Conv2d(2*self.z_channels, 2*self.embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(self.embed_dim, self.z_channels, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def encode(self, x, y):\n",
    "        h = self.encoder(x, y)\n",
    "        moments = self.quant_conv(h)\n",
    "        posterior = DiagonalGaussianDistribution(moments)\n",
    "        return posterior\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        z = self.post_quant_conv(z)\n",
    "        dec = self.decoder(z, y)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input, label, sample_posterior=True):\n",
    "        posterior = self.encode(input, label)\n",
    "        if sample_posterior:\n",
    "            z = posterior.sample()\n",
    "        else:\n",
    "            z = posterior.mode()\n",
    "        dec = self.decode(z, label)\n",
    "        return dec, posterior\n",
    "\n",
    "    def get_input(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        if x.shape[-1] != self.image_size:\n",
    "            x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
    "        else:\n",
    "            x = x.to(memory_format=torch.contiguous_format).float()\n",
    "        return x\n",
    "\n",
    "    def training_step(self, x, y, optimizer_idx):\n",
    "        inputs = self.get_input(x)\n",
    "        reconstructions, posterior = self(inputs, y)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # train encoder+decoder+logvar\n",
    "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "            \n",
    "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
    "            return aeloss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # train the discriminator\n",
    "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
    "                                                last_layer=self.get_last_layer(), split=\"train\")\n",
    "\n",
    "            \n",
    "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
    "            return discloss\n",
    "\n",
    "    def validation_step(self, x, y):\n",
    "        inputs = self.get_input(x)\n",
    "        reconstructions, posterior = self(inputs, y)\n",
    "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n",
    "                                        last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        \n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        return self.log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
    "                                  list(self.decoder.parameters())+\n",
    "                                  list(self.quant_conv.parameters())+\n",
    "                                  list(self.post_quant_conv.parameters()),\n",
    "                                  lr=lr, betas=(0.5, 0.9))\n",
    "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
    "                                    lr=lr, betas=(0.5, 0.9))\n",
    "        return opt_ae, opt_disc\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.final_layer.linear.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickit/miniforge3/envs/prj/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nickit/miniforge3/envs/prj/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "latent_dim = (16,16,3)\n",
    "vae = TAutoencoderKL(latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generate a tensor of (1,3,32,32)\n",
    "a = torch.randn(1,3,32,32)\n",
    "# randomly generate a label tensor of type long\n",
    "y = torch.randint(0,10,(1,)).long()\n",
    "b = vae(a, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickit/miniforge3/envs/prj/lib/python3.11/site-packages/pytorch_lightning/core/module.py:407: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5142.1221, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.training_step(a, y, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
